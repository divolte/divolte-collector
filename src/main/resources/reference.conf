//
// Copyright 2014 GoDataDriven B.V.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

// This is the default configuration.
divolte {
  server {
    // The host to which the server binds.
    // Set to a specific IP address to selectively listen on that interface.
    host = 0.0.0.0
    // The bind host can be overridden using the DIVOLTE_HOST environment variable.
    host = ${?DIVOLTE_HOST}

    // The port on which the sever listens.
    port = 8290
    // Server port can be overridden using the DIVOLTE_PORT environment variable.
    port = ${?DIVOLTE_PORT}

    // Whether to use the X-Forwarded-For header HTTP header
    // for determining the source IP of a request if present.
    // When a X-Forwared-For header is present, the rightmost
    // IP address of the value is used as source IP when
    // when multiple IP addresses are separated by a comma.
    // When the header is present more than once, the last
    // value will be used.
    // E.g.
    // "X-Forwarded-For: 10.200.13.28, 11.45.82.30" ==> 11.45.82.30
    //
    // "X-Forwarded-For: 10.200.13.28"
    // "X-Forwarded-For: 11.45.82.30" ==> 11.45.82.30
    use_x_forwarded_for = false

    // When true Divolte Collector serves a static test page at /.
    serve_static_resources = true
  }

  // The tracking section controls the settings related to the tracking
  // JavaScript. This script is compiled using the closure compiler
  // (https://developers.google.com/closure/compiler/) on startup of the
  // server. During compilation the values from the settings are substituted
  // in the JavaScript and thus appear as hard-coded on the client side.
  tracking {
    // The name of the cookie used for setting a party ID
    party_cookie = _dvp
    // The expiry time for the party ID cookie
    party_timeout = 730 days

    // The name of the cookie used tracking the session ID
    session_cookie = _dvs

    // The expiry time for a session
    session_timeout = 30 minutes

    // The cookie domain that is assigned to the cookies.
    // When left empty, the cookie will have no domain
    // explicitly associated with it, which effectively
    // sets it to the website domain of the page that
    // contains the Divolte Collector JavaScript.
    // cookie_domain = ''


    // This section controls the user agent parsing settings. The user agent
    // parsing is based on this library (https://github.com/before/uadetector),
    // which allows for dynamic reloading of the backing database if a internet
    // connection is available. The parser type controls this behavior.
    // Possible values are:
    // - non_updating:         Uses a local database, bundled
    //                         with Divolte Collector.
    // - online_updating:      Uses a online database only, never falls back
    //                         to the local database.
    // - caching_and_updating: Uses a cached version of the online database
    //                         and periodically checks for new version at the
    //                         remote location. Updates are downloaded
    //                         automatically and cached locally.

    ua_parser {
      // The parser type.
      type = non_updating

      // User agent parsing is a relatively expensive operation that requires
      // many regular expression evaluations. Very often the same user agent
      // will make consecutive requests and many clients will have the exact
      // same user agent as well. It therefore makes sense to cache the
      // parsing results in memory and do a lookup before trying a parse.
      // This setting determines how many unique user agent strings will be
      // cached.
      cache_size = 1000
    }

    // This configures the ip2geo database for geo lookups. A ip2geo database
    // can be obtained from MaxMind (https://www.maxmind.com/en/geoip2-databases).
    // Both a free version and a more accurate paid version are available.
    //
    // By default, no ip2geo database is configured. When this setting is
    // absent, no attempt will be made to lookup geo-coordinates for IP
    // addresses. If configured, Divolte Collector will keep a filesystem
    // watch on the database file. If the file is changed on the filesystem
    // the database will be reloaded at runtime without requireing a restart.
    // ip2geo_database = /path/to/dabase/file.db

    // By default, Divolte Collector will use a built-in Avro schema for
    // writing data and a default mapping, which is documented in the
    // Mapping section of the user documentation. The default schema
    // can be found here: https://github.com/divolte/divolte-schema
    //
    // Typically, users will configure their own schema, usually with
    // fields specific to their domain and custom events and other
    // mappings. When using a user defined schema, it is also
    // required to provide a mapping script. See the user documentation
    // for further reference.

    // schema_file = /Users/friso/code/divolte-examples/avro-schema/src/main/resources/JavadocEventRecord.avsc
    // schema_mapping {
         // The version of the mapping dialect to use. The current latest
         // version is 2. Version 1 has been deprecated and removed from
         // Divolte Collector since release 0.2
    //   version = 2

         // The groovy script file to use as mapping definition.
    //   mapping_script_file = "/Users/friso/code/divolte-examples/avro-schema/mapping.groovy"
    // }
  }

  // The javascript section controls settings related to the way
  // the JavaScript file is compiled.
  javascript {
    // Name of the script file. This changes the divolte.js part in
    // the script url: http://www.domain.tld/divolte.js
    name = divolte.js

    // Enable or disable the logging on the JavaScript console in
    // the browser
    logging = false

    // When true, the served JavaScript will be compiled, but not
    // minified, improving readability when debugging in the browser.
    debug = false
    
    // When false, divolte.js will not automatically send a pageView
    // event after being loaded. This way clients can send a initial
    // event themselves and have full control over the event type and
    // the custom parameters that are sent with the initial event. 
    auto_page_view_event = true
  }

  // This section controls settings related to the processing of incoming
  // requests after they have been responded to by the server. Incoming
  // requests in Divolte Collector are initially handled by a pool of
  // HTTP threads, which immediately respond with a HTTP code 200 and send
  // the response payload (a 1x1 pixel transparent GIF image). After
  // responding, the request data is passed onto the incoming request
  // processing thread pool. This is the incoming request processor.
  incoming_request_processor {
    // Number of threads to use for processing incoming requests
    threads = 2

    // The maximum queue of incoming requests to keep
    // before starting to drop incoming requests. Note
    // that when this queue is full, requests are dropped
    // and a warning is logged. No errors are reported to
    // the client side. Divolte Collector will always respond
    // with a HTTP 200 status code and the image payload.
    // Note that the queue size is per thread.
    max_write_queue = 100000

    // The maximum delay to block before an incoming request
    // is dropped in case of a full queue.
    max_enqueue_delay = 1 second

    // The incoming request handler attempts to parse out all
    // relevant information from the request as passed by the
    // JavaScript. If the incoming request appears corrupt,
    // for example because of a truncated URL or incorrect
    // data in the fields, the request is flagged as corrupt.
    // The detection of corrupt requests is enforced by appending
    // a hash of all fields to the request from the JavaScript.
    // This hash is validated on the server side.
    // If this setting is true, events that are flagged as corrupt
    // will be dropped from the stream, instead of processed further.
    // It is common not to drop the corrupt events, but instead
    // include them for later analysis.
    discard_corrupted = false

    // Browsers and other clients (e.g. anti-virus software, proxies)
    // will sometimes send the exact same request twice. Divolte
    // Collector attempts to flag these duplicate events, by using
    // a internal probabilistic data structure with a finite memory
    // size. The memory consists internally of an array of 64 bit
    // integers. This the memory required in bytes is the memory size
    // times 8 (8 megabytes for 1 million entries).
    // Note that the memory size is per thread.
    duplicate_memory_size = 1000000

    // If this setting is true, events that are flagged as duplicate
    // will be dropped from the stream, instead of processed further.
    // It is common not to drop the duplicate events, but instead
    // include them for later analysis.
    discard_duplicates = false
  }

  // This section controls settings related to flushing the event stream
  // to a Apache Kafka topic.
  kafka_flusher {
    // If true, flushing to Kafka is enabled.
    enabled = false

    // Number of threads to use for flushing events to Kafka
    threads = 2

    // The maximum queue of incoming requests to keep
    // before starting to drop incoming requests. Note
    // that when this queue is full, requests are dropped
    // and a warning is logged. No errors are reported to
    // the client side. Divolte Collector will always respond
    // with a HTTP 200 status code and the image payload.
    // Note that the queue size is per thread.
    max_write_queue = 200000

    // The maximum delay to block before an incoming request
    // is dropped in case of a full queue.
    max_enqueue_delay = 1 second

    // The Kafka topic onto which events are published.
    topic = "divolte"
    // The topic can be overridden by setting the
    // DIVOLTE_KAFKA_TOPIC environment variable.
    topic = ${?DIVOLTE_KAFKA_TOPIC}

    // All settings in here are used as-is to configure
    // the Kafka producer.
    // See: http://kafka.apache.org/documentation.html#producerconfigs
    producer = {
      metadata.broker.list = ["localhost:9092"]
      metadata.broker.list = ${?DIVOLTE_KAFKA_BROKER_LIST}

      client.id = divolte.collector
      client.id = ${?DIVOLTE_KAFKA_CLIENT_ID}

      request.required.acks = 0
      message.send.max.retries = 5
      retry.backoff.ms = 200
    }
  }

  // This section controls settings related to flushing the event stream
  // to HDFS.
  hdfs_flusher {
    // If true, flushing to HDFS is enabled.
    enabled = true

    // Number of threads to use for flushing events to HDFS.
    // Each thread creates its own files on HDFS. Depending
    // on the flushing strategy, multiple concurrent files
    // could be kept open per thread.
    threads = 2

    // The maximum queue of incoming requests to keep
    // before starting to drop incoming requests. Note
    // that when this queue is full, requests are dropped
    // and a warning is logged. No errors are reported to
    // the client side. Divolte Collector will always respond
    // with a HTTP 200 status code and the image payload.
    // Note that the queue size is per thread.
    max_write_queue = 100000

    // The maximum delay to block before an incoming request
    // is dropped in case of a full queue.
    max_enqueue_delay = 1 second


    // HDFS specific settings. Although it's possible to configure
    // a HDFS URI here, it is more advisable to configure HDFS
    // settings by specifying a HADOOP_CONF_DIR environment variable
    // which will be added to the classpath on startup and as such
    // configure the HDFS client automatically.
    hdfs {
      // default nonexistant: Use HADOOP_CONF_DIR on the classpath.
      // If not present empty config results in local filesystem being used.
      // uri = "file:///"
      // uri = ${?DIVOLTE_HDFS_URI}

      // The HDFS replication factor to use when creating
      // files.
      replication = 1

      // The replication factor can be overridden by setting the
      // DIVOLTE_HDFS_REPLICATION environment variable.
      replication = ${?DIVOLTE_HDFS_REPLICATION}
    }

    // Divolte Collector has two strategies for creating files
    // on HDFS and flushing data. By default, a simple rolling
    // file strategy is empoyed. This opens one file per thread
    // and rolls on to a new file after a configurable interval.
    // Files that are being written to, have a extension of
    // .avro.partial and are written the the directory configured
    // in the working_dir setting. When a file is closed, it
    // will be renamed to have a .avro extension and is moved to
    // the directory configured in the publish_dir settins. This
    // happens in a single (atomic) filesystem move operation.
    simple_rolling_file_strategy {
      // Roll over files on HDFS after this amount of time.
      roll_every = 60 minutes

      // Issue a hsync against files each time this number of
      // records has been flushed to it.
      sync_file_after_records = 1000

      // If no records are being flushed, issue a hsync when
      // this amount of time passes, regardless of how much
      // data was written.
      sync_file_after_duration = 30 seconds

      // Directory where files are created and kept while being
      // written to.
      working_dir = /tmp

      // Directory where files are moved to, after they are closed.
      publish_dir = /tmp
    }

    // Next to the rolling file strategy, there is a more complex
    // strategy called session binning file strategy. The general
    // idea of this strategy is to provide a best effort to put
    // events that belong to the same session in the same file.
    //
    // This strategy assigns event to files as such:
    // - Each event is assigned to a round based on timestamp,
    //   defined as timestamp_in_millis / session_timeout_in_millis.
    // - A file is opened for each round as time passes.
    // - All events for a session are stored in the file with the
    //   round marked by the session start time.
    // - A file for a round is kept open for at least three times the
    //   session duration *in absence of failures*.
    // - During this entire process, the event timestamp is used for
    //   events that come off the queue as a logical clock signal.
    //      - Only in the case of an empty queue, the actual system
    //        time is used as clock signal.
    // - When a file for a round is closed, but events that should be
    //   in that file still arrive, they are stored in the oldest open
    //   file.
    //      - This happens for exceptionally long sessions
    //
    // This strategy attempts to write events that belong to the same
    // session to the same file. Do note that in case of failures,
    // this guarantee not longer holds. For this reason, in failure
    // scenario's or at shutdown, this strategy DOES NOT move files
    // to the publish directory. Users have to setup a separate process
    // to periodically move these files out of the way.

//    session_binning_file_strategy {
//      sync_file_after_records = 1
//      sync_file_after_duration = 1 hour
//    }

  }
}
